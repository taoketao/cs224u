{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bake-off: Word similarity tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2018 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "0. [Overview](#Overview)\n",
    "0. [Evaluation](#Evaluation)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Dataset readers](#Dataset-readers)\n",
    "0. [Evaluation](#Evaluation)\n",
    "0. [Baseline](#Baseline)\n",
    "0. [Bake-off submission](#Bake-off-submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Word similarity datasets have long been used to evaluate distributed representations. This section provides basic code for conducting such analyses with four datasets:\n",
    "\n",
    "* [WordSim-353](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)\n",
    "* MTurk-287\n",
    "* [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html)\n",
    "* [The MEN Test Collection](http://clic.cimec.unitn.it/~elia.bruni/MEN) (3,000 pairs)\n",
    "\n",
    "For the first three, the numeral in its name is the number of pairs it contains.\n",
    "\n",
    "If you want to push this task further, consider using additional datasets from http://wordvectors.org/ and perhaps even taking advantage of the evaluation infrastructure it provides. (For additional details, see [the associated paper](http://www.aclweb.org/anthology/P/P14/P14-5004.pdf).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "1. Each of the similarity datasets contains word pairs with an associated human-annotated similarity score. (We convert these to distances to align intuitively with our distance measure functions.)\n",
    "\n",
    "1. The evaluation code measures the distance between the word pairs in your chosen VSM (should be a `pd.DataFrame`).\n",
    "\n",
    "1. The evaluation metric is the Spearman correlation coefficient between the annotated scores and your distances.\n",
    "\n",
    "1. We also macro-average these correlations across the four datasets for an overall summary.\n",
    "\n",
    "Based on my reading of the literature, I'd say that the best VSMs report scores in this range:\n",
    "\n",
    "| Dataset       | Competitive scores |\n",
    "|---------------|--------------------|\n",
    "| WordSim-353   | ≈0.75        |\n",
    "| MTurk-287     | ≈0.75        |\n",
    "| MTurk-771     | ≈0.75        |\n",
    "| MEN           | ≈0.70        |\n",
    "\n",
    "Your scores won't quite be comparable because you'll be missing a few vocabulary items, but these are still good targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import vsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = 'vsmdata'\n",
    "\n",
    "wordsim_home = os.path.join('vsmdata', 'wordsim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsim_dataset_reader(src_filename, header=False, delimiter=','):    \n",
    "    \"\"\"Basic reader that works for all four files, since they all have the \n",
    "    format word1,word2,score, differing only in whether or not they include \n",
    "    a header line and what delimiter they use.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the source file.        \n",
    "    header : bool (default: False)\n",
    "        Whether `src_filename` has a header.        \n",
    "    delimiter : str (default: ',')\n",
    "        Field delimiter in `src_filename`.\n",
    "    \n",
    "    Yields\n",
    "    ------    \n",
    "    (str, str, float)\n",
    "       (w1, w2, score) where `score` is the negative of the similarity \n",
    "       score in the file so that we are intuitively aligned with our \n",
    "       distance-based code.\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(src_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "        if header:\n",
    "            next(reader)\n",
    "        for row in reader:\n",
    "            w1, w2, score = row\n",
    "            # Negative of scores to align intuitively with distance functions:\n",
    "            score = -float(score)\n",
    "            yield (w1, w2, score)\n",
    "\n",
    "def wordsim353_reader():\n",
    "    \"\"\"WordSim-353: http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\"\"\"\n",
    "    src_filename = os.path.join(wordsim_home, 'wordsim353.csv')\n",
    "    return wordsim_dataset_reader(src_filename, header=True)\n",
    " \n",
    "def mturk287_reader():\n",
    "    \"\"\"MTurk-287: http://tx.technion.ac.il/~kirar/Datasets.html\"\"\"\n",
    "    src_filename = os.path.join(wordsim_home, 'MTurk-287.csv')\n",
    "    return wordsim_dataset_reader(src_filename, header=False)\n",
    "    \n",
    "def mturk771_reader():\n",
    "    \"\"\"MTURK-771: http://www2.mta.ac.il/~gideon/mturk771.html\"\"\"\n",
    "    src_filename = os.path.join(wordsim_home, 'MTURK-771.csv')\n",
    "    return wordsim_dataset_reader(src_filename, header=False)\n",
    "\n",
    "def men_reader():\n",
    "    \"\"\"MEN: http://clic.cimec.unitn.it/~elia.bruni/MEN\"\"\"\n",
    "    src_filename = os.path.join(wordsim_home, 'MEN_dataset_natural_form_full')\n",
    "    return wordsim_dataset_reader(src_filename, header=False, delimiter=' ')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity_evaluation(reader, df, distfunc=vsm.cosine, verbose=True):\n",
    "    \"\"\"Word-similarity evalution framework.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    reader : iterator\n",
    "        A reader for a word-similarity dataset. Just has to yield\n",
    "        tuples (word1, word2, score).    \n",
    "    df : pd.DataFrame\n",
    "        The VSM being evaluated.        \n",
    "    distfunc : function mapping vector pairs to floats (default: `vsm.cosine`)\n",
    "        The measure of distance between vectors. Can also be `vsm.euclidean`, \n",
    "        `vsm.matching`, `vsm.jaccard`, as well as any other distance measure \n",
    "        between 1d vectors.  \n",
    "    verbose : bool\n",
    "        Whether to print information about how much of the vocab\n",
    "        `df` covers.\n",
    "    \n",
    "    Prints\n",
    "    ------\n",
    "    To standard output\n",
    "        Size of the vocabulary overlap between the evaluation set and\n",
    "        rownames. We limit the evalation to the overlap, paying no price\n",
    "        for missing words (which is not fair, but it's reasonable given\n",
    "        that we're working with very small VSMs in this notebook).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Spearman rank correlation coefficient between the dataset\n",
    "        scores and the similarity values obtained from `mat` using \n",
    "        `distfunc`. This evaluation is sensitive only to rankings, not\n",
    "        to absolute values.\n",
    "    \n",
    "    \"\"\"    \n",
    "    sims = defaultdict(list)\n",
    "    rownames = df.index\n",
    "    vocab = set()    \n",
    "    excluded = set()\n",
    "    for w1, w2, score in reader():\n",
    "        if w1 in rownames and w2 in rownames:\n",
    "            sims[w1].append((w2, score))\n",
    "            sims[w2].append((w1, score))\n",
    "            vocab |= {w1, w2}\n",
    "        else:\n",
    "            excluded |= {w1, w2}\n",
    "    all_words = vocab | excluded\n",
    "    if verbose:\n",
    "        print(\"Evaluation vocab: {:,} of {:,}\".format(len(vocab), len(all_words)))\n",
    "    # Evaluate the matrix by creating a vector of all_scores for data\n",
    "    # and all_dists for mat's distances. \n",
    "    all_scores = []\n",
    "    all_dists = []\n",
    "    for word in vocab:\n",
    "        vec = df.loc[word]\n",
    "        vals = sims[word]\n",
    "        cmps, scores = zip(*vals)\n",
    "        all_scores += scores\n",
    "        all_dists += [distfunc(vec, df.loc[w]) for w in cmps]\n",
    "    rho, pvalue = spearmanr(all_scores, all_dists)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation is then simple. The following lets us evaluate a VSM against all four datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_word_similarity_evaluation(df, verbose=True):\n",
    "    \"\"\"Evaluate a VSM against all four datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping dataset names to Spearman r values\n",
    "        \n",
    "    \"\"\"        \n",
    "    scores = {}\n",
    "    for reader in (wordsim353_reader, mturk287_reader, mturk771_reader, men_reader):        \n",
    "        if verbose: \n",
    "            print(\"=\"*40)\n",
    "            print(reader.__name__)\n",
    "        score = word_similarity_evaluation(reader, df, verbose=verbose)\n",
    "        scores[reader.__name__] = score\n",
    "        if verbose:            \n",
    "            print('Spearman r: {0:0.03f}'.format(score))\n",
    "    mu = np.array(list(scores.values())).mean()\n",
    "    if verbose:\n",
    "        print(\"=\"*40)\n",
    "        print(\"Mean Spearman r: {0:0.03f}\".format(mu))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "My baseline is PPMI on `imdb20`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb20 = pd.read_csv(\n",
    "    os.path.join(data_home, \"imdb_window20-flat.csv.gz\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb20_ppmi = vsm.pmi(imdb20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "wordsim353_reader\n",
      "Evaluation vocab: 418 of 437\n",
      "Spearman r: 0.469\n",
      "========================================\n",
      "mturk287_reader\n",
      "Evaluation vocab: 499 of 499\n",
      "Spearman r: 0.599\n",
      "========================================\n",
      "mturk771_reader\n",
      "Evaluation vocab: 1,113 of 1,113\n",
      "Spearman r: 0.462\n",
      "========================================\n",
      "men_reader\n",
      "Evaluation vocab: 751 of 751\n",
      "Spearman r: 0.572\n",
      "========================================\n",
      "Mean Spearman r: 0.525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'men_reader': 0.5724487594295152,\n",
       " 'mturk287_reader': 0.5986597600532505,\n",
       " 'mturk771_reader': 0.4615212813179131,\n",
       " 'wordsim353_reader': 0.46888766456156583}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_word_similarity_evaluation(imdb20_ppmi, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off submission\n",
    "\n",
    "1. The name of the count matrix you started with (must be one in `vsmdata`).\n",
    "1. A description of the steps you took to create your bake-off VSM – must be different from the above baseline.\n",
    "1. Your Spearman r value for each of the four evaluation datasets and your average across all four.\n",
    "\n",
    "Submission URL: https://goo.gl/forms/eJWHpwJlOyDEC3V63"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
